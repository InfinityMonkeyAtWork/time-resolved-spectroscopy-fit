{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# ML Training Data Generation with Parameter Sweeps\n",
    "\n",
    "This notebook demonstrates how to generate synthetic training datasets for machine learning by systematically varying model parameters.\n",
    "\n",
    "**Use cases:**\n",
    "- Training neural networks for spectroscopy analysis\n",
    "- Testing parameter recovery algorithms\n",
    "- Exploring parameter sensitivity and identifiability\n",
    "- Generating data for uncertainty quantification studies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import trspecfit\n",
    "from trspecfit.utils.sweep import ParameterSweep, SweepDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model_setup",
   "metadata": {},
   "source": [
    "## 1. Set Up Model\n",
    "\n",
    "First, create your model as usual. This will be the \"ground truth\" model that we'll vary to generate training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create_project",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create parent project\n",
    "project = trspecfit.Project(path=os.getcwd())\n",
    "\n",
    "# Create file instance with axes\n",
    "file = trspecfit.File(\n",
    "    parent_project=project,\n",
    "    energy=np.arange(0, 20, 0.01),\n",
    "    time=np.arange(-10, 100, 0.1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load_energy_model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load energy-resolved model\n",
    "file.load_model(\n",
    "    model_yaml='models_energy.yaml',\n",
    "    model_info=['single_peak']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add_dynamics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add time dependence to make it 2D\n",
    "file.add_time_dependence(\n",
    "    model_yaml=\"models_time.yaml\",\n",
    "    model_info=['MonoExpPosIRF'],\n",
    "    par_name=\"GLP_01_x0\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "describe_model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the model structure\n",
    "file.model_active.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parameter_space_header",
   "metadata": {},
   "source": [
    "## 2. Define Parameter Space\n",
    "\n",
    "Choose which parameters to vary and how. You can use:\n",
    "- **Discrete ranges**: `add_range()` for specific values\n",
    "- **Continuous distributions**: `add_uniform()`, `add_normal()`, `add_lognormal()`\n",
    "\n",
    "Strategy options:\n",
    "- **`'grid'`**: Full combinatorial - every combination (good for small spaces)\n",
    "- **`'random'`**: Independent sampling (scalable for ML)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grid_example_header",
   "metadata": {},
   "source": [
    "### Example 1: Small Grid Sweep\n",
    "\n",
    "Good for systematic exploration of a few parameter values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grid_sweep",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a small grid\n",
    "sweep_grid = ParameterSweep(strategy='grid', seed=42)\n",
    "sweep_grid.add_range('GLP_01_x0_expFun_01_A', [0.25, 1, 4])\n",
    "sweep_grid.add_uniform('GLP_01_x0_expFun_01_tau', min_val=5, max_val=25, n_samples=3)\n",
    "\n",
    "print(f\"Total configurations: {sweep_grid.get_n_configs()}\")\n",
    "print(\"\\nFirst 5 configurations:\")\n",
    "for i, config in enumerate(sweep_grid):\n",
    "    if i >= 5:\n",
    "        break\n",
    "    print(f\"  Config {i}: {config}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "random_example_header",
   "metadata": {},
   "source": [
    "### Example 2: Random Sampling (Recommended for ML)\n",
    "\n",
    "Samples parameters independently - scales to large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "random_sweep",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define random sampling sweep\n",
    "sweep_random = ParameterSweep(strategy='random', seed=42)\n",
    "\n",
    "# Vary peak amplitude\n",
    "sweep_random.add_uniform('GLP_01_A', min_val=8, max_val=12, n_samples=100)\n",
    "\n",
    "# Vary peak position\n",
    "sweep_random.add_uniform('GLP_01_x0', min_val=8, max_val=12, n_samples=100)\n",
    "\n",
    "# Vary dynamics amplitude\n",
    "sweep_random.add_uniform('GLP_01_x0_expFun_01_A', min_val=0.5, max_val=5, n_samples=100)\n",
    "\n",
    "# Vary decay time constant\n",
    "sweep_random.add_uniform('GLP_01_x0_expFun_01_tau', min_val=10, max_val=80, n_samples=100)\n",
    "\n",
    "print(f\"Total configurations: {sweep_random.get_n_configs()}\")\n",
    "print(\"\\nFirst 5 configurations:\")\n",
    "for i, config in enumerate(sweep_random):\n",
    "    if i >= 5:\n",
    "        break\n",
    "    print(f\"  Config {i}:\")\n",
    "    for k, v in config.items():\n",
    "        print(f\"    {k}: {v:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "simulation_header",
   "metadata": {},
   "source": [
    "## 3. Generate Training Data\n",
    "\n",
    "Use `simulate_parameter_sweep()` to generate datasets. This:\n",
    "1. Iterates through parameter configurations\n",
    "2. Updates model parameters\n",
    "3. Generates N noisy realizations per configuration\n",
    "4. Saves everything to HDF5 file\n",
    "\n",
    "**Memory efficiency**: Only one configuration in memory at a time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create_simulator",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create simulator\n",
    "sim = trspecfit.Simulator(\n",
    "    model=file.model_active,\n",
    "    detection='analog',\n",
    "    noise_level=0.05,\n",
    "    noise_type='poisson',\n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "small_sweep_header",
   "metadata": {},
   "source": [
    "### Small Test Dataset (Quick)\n",
    "\n",
    "Generate a small dataset first to verify everything works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "small_sweep",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the grid sweep for quick test\n",
    "sim.simulate_parameter_sweep(\n",
    "    parameter_sweep=sweep_grid,\n",
    "    N_realizations=3,\n",
    "    filepath='test_sweep.h5',\n",
    "    show_progress=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "large_sweep_header",
   "metadata": {},
   "source": [
    "### Large Training Dataset (Takes Time)\n",
    "\n",
    "Generate the full ML training dataset. Adjust N_realizations based on your needs:\n",
    "- **N=10-20**: Good for most ML applications\n",
    "- **N=50+**: For robust uncertainty quantification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "large_sweep",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate large training dataset\n",
    "# WARNING: This can take up to several minutes!\n",
    "sim.simulate_parameter_sweep(\n",
    "    parameter_sweep=sweep_random,\n",
    "    N_realizations=20,\n",
    "    filepath='ml_training_data.h5',\n",
    "    show_progress=True\n",
    ")\n",
    "\n",
    "# Result: 100 configs Ã— 20 realizations = 2000 training examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inspection_header",
   "metadata": {},
   "source": [
    "## 4. Inspect Generated Data\n",
    "\n",
    "Use `SweepDataset` to examine what was generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load_dataset",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset inspector\n",
    "dataset = SweepDataset('simulated_data/ml_training_data.h5')\n",
    "#dataset = SweepDataset('simulated_data/test_sweep.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "print_summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print comprehensive summary\n",
    "dataset.print_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parameter_summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get parameter summary as DataFrame\n",
    "df = dataset.get_parameter_summary()\n",
    "print(\"\\nParameter configurations:\")\n",
    "display(df.round(3))\n",
    "\n",
    "print(\"\\nStatistical summary:\")\n",
    "display(df.describe().round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize_parameters",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize parameter distributions\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df.hist(figsize=(12, 8), bins=20)\n",
    "plt.suptitle('Parameter Distributions in Training Data')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inspect_config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a specific configuration for inspection\n",
    "config = dataset.load_config(0)\n",
    "\n",
    "print(f\"Config 0 parameters:\")\n",
    "for k, v in config['parameters'].items():\n",
    "    print(f\"  {k}: {v:.3f}\")\n",
    "\n",
    "print(f\"\\nData shapes:\")\n",
    "print(f\"  Clean: {config['clean'].shape}\")\n",
    "print(f\"  Noisy realizations: {len(config['noisy'])}\")\n",
    "print(f\"  Each realization: {config['noisy'][0].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize_data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize a configuration (uses simulator's built-in plotting)\n",
    "print(\"\\nVisualize configuration 0:\")\n",
    "print(f\"  - Or use your own plotting with dataset.get_axes() for x/y axes\")\n",
    "print(f\"  - Or (see below): Load config, set sim.data_clean/noisy, call sim.plot_comparison(dim=2)\")\n",
    "# Temporarily set simulator data to this config for plotting\n",
    "sim.data_clean = config['clean']\n",
    "sim.data_noisy = config['noisy'][0]\n",
    "sim.noise = config['noisy'][0] - config['clean']\n",
    "sim.plot_comparison(dim=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ml_usage_header",
   "metadata": {},
   "source": [
    "## 5. Loading Data for ML Training\n",
    "\n",
    "For actual ML training, you'll typically write a custom data loader for your framework (PyTorch, TensorFlow, etc.). Here's a basic example using h5py directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ml_loader_example",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "# Example: Load all data into memory (for small datasets)\n",
    "def load_training_data(filepath, param_names):\n",
    "    \"\"\"\n",
    "    Load parameter sweep data for ML training.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    filepath : str\n",
    "        Path to HDF5 file\n",
    "    param_names : list of str\n",
    "        Which parameters to extract as targets\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    X : list of arrays\n",
    "        Input spectra (noisy realizations)\n",
    "    y : list of arrays\n",
    "        Target parameters for each spectrum\n",
    "    \"\"\"\n",
    "    X = []  # Input: noisy spectra\n",
    "    y = []  # Target: parameter values\n",
    "    \n",
    "    with h5py.File(filepath, 'r') as f:\n",
    "        n_configs = f['metadata'].attrs['n_configs']\n",
    "        \n",
    "        for config_idx in range(n_configs):\n",
    "            config_name = f'config_{config_idx:06d}'\n",
    "            config_group = f['parameter_configs'][config_name]\n",
    "            data_group = f['simulated_data'][config_name]\n",
    "            \n",
    "            # Extract target parameters\n",
    "            targets = [config_group.attrs[name] for name in param_names]\n",
    "            \n",
    "            # Load all noisy realizations for this config\n",
    "            for real_key in sorted(data_group.keys()):\n",
    "                spectrum = data_group[real_key][:]\n",
    "                X.append(spectrum)\n",
    "                y.append(targets)\n",
    "    \n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Example usage\n",
    "X_train, y_train = load_training_data(\n",
    "    'simulated_data/test_sweep.h5',\n",
    "    param_names=['GLP_01_x0_expFun_01_A', 'GLP_01_x0_expFun_01_tau']\n",
    ")\n",
    "\n",
    "print(f\"Training data shape: {X_train.shape}\")\n",
    "print(f\"Target shape: {y_train.shape}\")\n",
    "print(f\"\\nFirst example target: {y_train[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pytorch_example_header",
   "metadata": {},
   "source": [
    "### Example: PyTorch Dataset Class\n",
    "\n",
    "For large datasets that don't fit in memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pytorch_example",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment if you have PyTorch installed:\n",
    "# import torch\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "# \n",
    "# class SpectroscopyDataset(Dataset):\n",
    "#     \"\"\"PyTorch dataset for parameter sweep data.\"\"\"\n",
    "#     \n",
    "#     def __init__(self, filepath, param_names, transform=None):\n",
    "#         self.filepath = filepath\n",
    "#         self.param_names = param_names\n",
    "#         self.transform = transform\n",
    "#         \n",
    "#         # Load metadata\n",
    "#         with h5py.File(filepath, 'r') as f:\n",
    "#             self.n_configs = f['metadata'].attrs['n_configs']\n",
    "#             self.n_realizations = f['metadata'].attrs['n_realizations_per_config']\n",
    "#         \n",
    "#         self.total_samples = self.n_configs * self.n_realizations\n",
    "#     \n",
    "#     def __len__(self):\n",
    "#         return self.total_samples\n",
    "#     \n",
    "#     def __getitem__(self, idx):\n",
    "#         # Calculate which config and realization\n",
    "#         config_idx = idx // self.n_realizations\n",
    "#         real_idx = idx % self.n_realizations\n",
    "#         \n",
    "#         with h5py.File(self.filepath, 'r') as f:\n",
    "#             config_name = f'config_{config_idx:06d}'\n",
    "#             config_group = f['parameter_configs'][config_name]\n",
    "#             data_group = f['simulated_data'][config_name]\n",
    "#             \n",
    "#             # Load spectrum\n",
    "#             spectrum = data_group[f'{real_idx:06d}'][:]\n",
    "#             \n",
    "#             # Load target parameters\n",
    "#             targets = [config_group.attrs[name] for name in self.param_names]\n",
    "#         \n",
    "#         if self.transform:\n",
    "#             spectrum = self.transform(spectrum)\n",
    "#         \n",
    "#         return torch.FloatTensor(spectrum), torch.FloatTensor(targets)\n",
    "# \n",
    "# # Create data loader\n",
    "# dataset = SpectroscopyDataset(\n",
    "#     'simulated_data/test_sweep.h5',\n",
    "#     param_names=['GLP_01_x0_expFun_01_A', 'GLP_01_x0_expFun_01_tau']\n",
    "# )\n",
    "# \n",
    "# train_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "# \n",
    "# # Use in training loop\n",
    "# for batch_spectra, batch_targets in train_loader:\n",
    "#     # ... training code ...\n",
    "#     pass\n",
    "\n",
    "print(\"See commented code above for PyTorch example\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tips_header",
   "metadata": {},
   "source": [
    "## Tips for ML Training Data\n",
    "\n",
    "**Parameter Selection:**\n",
    "- Start with 2-3 most important parameters\n",
    "- Use realistic ranges from your experimental data\n",
    "- Include correlations if parameters are physically related\n",
    "\n",
    "**Dataset Size:**\n",
    "- Small networks: 1,000-5,000 examples often sufficient\n",
    "- Larger networks: 10,000-50,000 examples\n",
    "- More realizations per config = better uncertainty estimates\n",
    "\n",
    "**Noise Level:**\n",
    "- Match your experimental SNR\n",
    "- Can generate multiple datasets with different noise levels\n",
    "- Consider augmentation during training (add more noise)\n",
    "\n",
    "**Validation Strategy:**\n",
    "- Split by configuration, not by realization\n",
    "- Test on held-out parameter ranges\n",
    "- Validate on real experimental data when possible"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
